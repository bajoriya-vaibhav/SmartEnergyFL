Microservice Based Architecture Design for Smart Energy Meter System Using Flower Framework

What is Federated Learning?
‚ÄãFederated Learning (FL) is a machine learning paradigm in which the model is training locally at the client side and then the updated weights are then sent to the server where they get aggregated to make the final global model which solves data privacy and security issues. 
Basically, FL is a decentralized approach to machine learning that enables multiple clients to collaboratively train a shared global model without exchanging their local datasets. This paradigm enhances data privacy and security by ensuring that sensitive information remains on local devices, with only model updates being communicated.

How it is Different from the Centralized Machine Learning Approach?
In traditional centralized machine learning, data from various sources is aggregated into a central server where the model training occurs. This approach poses significant privacy concerns. Federated Learning addresses these concerns by inverting the process: instead of moving data to the computation, it moves computation to the data. This means that model training happens locally on each client device, and only the model parameters or updates are sent back to a central server for aggregation.
How can we implement Federated Learning?
The FL process typically involves the following steps:
‚Äã 


Step 0(Initialize global model): initialise the global model parameters mostly after training the global model on some previous data or training data. 
Step 1(Send model to a number of client nodes): Now, each client devices have the global model parameters ensuring that every node is in sync. Note:-We often use only a few of the connected nodes instead of all nodes. The reason for this is that selecting more and more client nodes has diminishing returns. 
Step 2(Train model locally on the data of each client node): Then, each client will start training the model(latest one) on the local training dataset each node has collected. 
Note:- We don‚Äôt train the model until full convergence(overfitting may take place), but they only train for a little while. This could be as little as one epoch on the local data, or even just a few steps (mini-batches). 
Step 3: Return model updates back to the server: After local training, each client node sends its updated weights or gradients to the server. 
Step 4: Aggregate model updates into a new global model The server receives model updates from the selected client nodes. Then take the weighted average of the model updated by the number of samples each client used for training, this porcess is called as federated average(FedAvg). 
Step 5: Repeat steps 1 to 4 until the model converges. 

So, the process is like this (for my reference) Steps 1 to 4 are what we call a single round of federated learning. The global model parameters get sent to the participating client nodes (step 1), the client nodes train on their local data (step 2), they send their updated models to the server (step 3), and the server then aggregates the model updates to get a new version of the global model (step 4).Repeat till the model converges or give accurate results.
As we understood the workflow, now i will explain some important terminology that will be helpful when i will explain the flower framework and for code understanding.

Terminology Alert!!üôÇ

Federated Evaluation:
Just like we can train a model on the decentralized data of different client nodes, we can also evaluate the model on that data to receive valuable metrics.
Why it matters? Instead of bringing client data to a central server for evaluation (which may violate privacy), the evaluation is performed locally on each client. The results (like accuracy, precision, etc.) are aggregated, not the data itself.

Federated Analytics:
This involves running aggregate data queries across decentralized clients to gain insights without training a machine learning model.
For example, we can take the example where we don‚Äôt have that much data to get clear predictions like ‚ÄúWhat‚Äôs the average blood pressure reading across all participants?‚Äù
Here we are not getting enough data because of Privacy concerns, but for that we can get the aggregated results not individuals data hence not breaching anyones privacy.
Therefore Federated learning provides approach like secure aggregation which ensure that no single client's data is visible‚Äîonly the aggregate result.
In our Smart Energy Meter reading predictions also we cannot pass the individual data, therefore we have passed the aggregated predicted meter readings over the clients of a certain area so we can easily predict the consumption over that area without any breach in privacy.


Differential Privacy:
In this approach, we ensures individual-level data privacy by adding noise to results or updates before sharing.
This approach is used for flower internally while sending model updates or analytics from client to server to make sure individual client contributions can‚Äôt be reverse-engineered.

Now, we discuss about the framework we are using for our project to implement the Federated learning model for the Smart Meter System.

What is Flower?
Flower is an open-source framework for building federated learning (FL), federated evaluation, and federated analytics systems. Basically it provides the architecture design flow to move machine learning models from server to client(sending global model to client then after training sending the updated model back to the server from each client), training loop and evaluation cycle on local data, and then aggregating the updated models on server to make the updated global model. 
Basically it is design flow or tool to orchestrate communication between a centralized server and distributed client nodes, allowing easy integration with machine learning workflows and tools.

Flower Architecture

Figure reference link Flower architecture design

Flower Server design:
The server is made up of two key parts: SuperLink and ServerApp. SuperLink is a long-running background service that acts like a central hub ‚Äî it stays active all the time, sending out tasks to different devices (clients) and collecting their responses. We can think of it like a dispatcher or coordinator that keeps the communication flowing smoothly between the server and all the clients. Also as per the recent version of the Flower framework the server app is deprecated, therefore we have to use the superlink based architecture to run the application. Also it on its own it make the SSL/TLS grpc connection which we have to make own its own earlier. 
ServerApp is the main python script that all the federated learning operations like which clients to involve, how to configure them, and how to combine their results after training.

Flower Client design:
Each Flower client also has two parts: SuperNode and ClientApp. The SuperNode is like a helper that always stays on, waiting for instructions from the SuperLink. When a task arrives (like training a model on local data), it picks it up, runs it, and sends back the results. It‚Äôs like the persistent worker that makes sure the client is always ready.
ClientApp is the part that runs all the client-side federated learning operations like training or evaluating models on the device‚Äôs local data and then sending/loading the updated/global model weights to and from the client.

Why we need SuperNode and SuperLink? In federated learning,the architecture mainly involves server and client. But Superlink and Supernode orchestrates the flow in secure, reliable, efficient and organised manner. Therefore, it is named as SuperNode. The SuperLink is then responsible for acting as the missing link between all those SuperNodes.

In our project the devised architecture design is one Superlink with subprocess serverApp as server side and Supernode with subprocess clientApp as each client node.

Federated Learning Strategies:

Federated Learning (FL) strategies define how local model updates from multiple clients (devices or nodes) are aggregated into a global model during training. These strategies are central to coordinating learning across decentralized data sources while preserving privacy. Common FL strategies include FedAvg (Federated Averaging), where local models are averaged based on the number of local samples; FedProx, which adds regularization to handle data heterogeneity; and FedOpt, which applies optimization techniques like Adam or Yogi during aggregation. The choice of strategy directly impacts model performance, convergence speed, and robustness in real-world distributed environments.

Flower provide both the approaches to use their own strategy or implement a custom strategy which will have the Flower Strategy base class.

A few example which we tried for our model training:
CNN based FedAvg strategy which basically take all the model weights and then average over them to get the updated global model.


Figure reference link CNN architecture



Tree based FL aggregation strategy:
Tree-based Federated Learning (FL) aggregation is a hierarchical strategy where client updates are aggregated in a tree-like structure instead of a flat, centralized manner. In this approach, clients are grouped into clusters (or sub-trees), and each group elects an intermediate aggregator (e.g., edge server or SuperNode) to combine local updates before forwarding them up the hierarchy. This reduces communication overhead, enhances scalability, and allows partial model aggregation closer to data sources, making it suitable for large-scale or bandwidth-constrained federated networks. Tree-based aggregation can also improve fault tolerance and support asynchronous communication across layers.




